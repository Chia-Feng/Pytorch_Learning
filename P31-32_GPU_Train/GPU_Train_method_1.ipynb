{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from model import *\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data = torchvision.datasets.CIFAR10(\"../P28-30_mock_project_practice/data/train\", train=True, transform=torchvision.transforms.ToTensor(),download=True)\n",
    "##  training data has larger dataset which is used for train\n",
    "test_data = torchvision.datasets.CIFAR10(\"../P28-30_mock_project_practice/data/test\", train=False, transform=torchvision.transforms.ToTensor(),download=True)\n",
    "## smaller dataset, only for test the trained result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data set length: 50000\n",
      "test data set length: 10000\n"
     ]
    }
   ],
   "source": [
    "train_data_size = len(train_data)\n",
    "test_data_size = len(test_data)\n",
    "print(\"train data set length: {}\".format(train_data_size))\n",
    "print(\"test data set length: {}\".format(test_data_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data as batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 64\n",
    "train_dataloader = DataLoader(train_data, batch_size = batchsize)\n",
    "test_dataloader = DataLoader(test_data, batch_size = batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAR10model, self).__init__()\n",
    "        self.model1 = Sequential(\n",
    "          Conv2d(3, 32, 5, padding=2),\n",
    "          MaxPool2d(2),\n",
    "          Conv2d(32,32,5, padding=2),\n",
    "          MaxPool2d(2),\n",
    "          Conv2d(32,64,5, padding=2),\n",
    "          MaxPool2d(2),\n",
    "          Flatten(),\n",
    "          Linear(1024,64),             # be careful, the last step is linearing 1024 data to 64 then linear to 10\n",
    "          Linear(64,10)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output1 = self.model1(input)\n",
    "        return output1\n",
    "\n",
    "CIFAR10Model = CIFAR10model()\n",
    "\n",
    "if torch.cuda.is_available():                # if GPU is available -> use GPU for trainning\n",
    "  CIFAR10Model = CIFAR10Model.cuda()           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():                # if GPU is available -> use GPU for trainning\n",
    "  loss_fn = loss_fn.cuda()         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Learning_Rate = 0.01\n",
    "optimizer = torch.optim.SGD(CIFAR10Model.parameters(), lr = Learning_Rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_step = 0\n",
    "epoch = 5\n",
    "writer = SummaryWriter(\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.1 round trainning\n",
      "trained 4000 times, loss: 1.4154222011566162\n",
      "trained 4500 times, loss: 1.3150341510772705\n",
      "test 50 times, loss: 77.9490293264389\n",
      "Accuracy: 0.4362500011920929\n",
      "test 100 times, loss: 156.11858093738556\n",
      "Accuracy: 0.4364062547683716\n",
      "test 150 times, loss: 234.2795271873474\n",
      "Accuracy: 0.4345833361148834\n",
      "No.2 round trainning\n",
      "trained 5000 times, loss: 1.4145859479904175\n",
      "test 50 times, loss: 74.32747602462769\n",
      "Accuracy: 0.4556249976158142\n",
      "test 100 times, loss: 148.45159327983856\n",
      "Accuracy: 0.46187499165534973\n",
      "test 150 times, loss: 223.0605412721634\n",
      "Accuracy: 0.46041667461395264\n",
      "No.3 round trainning\n",
      "trained 5500 times, loss: 1.1655455827713013\n",
      "trained 6000 times, loss: 1.570720911026001\n",
      "test 50 times, loss: 71.30336248874664\n",
      "Accuracy: 0.48656249046325684\n",
      "test 100 times, loss: 142.18683099746704\n",
      "Accuracy: 0.4920312464237213\n",
      "test 150 times, loss: 213.97794806957245\n",
      "Accuracy: 0.48885416984558105\n",
      "No.4 round trainning\n",
      "trained 6500 times, loss: 1.60708487033844\n",
      "trained 7000 times, loss: 0.8939529657363892\n",
      "test 50 times, loss: 67.70962834358215\n",
      "Accuracy: 0.5171874761581421\n",
      "test 100 times, loss: 134.94717073440552\n",
      "Accuracy: 0.5209375023841858\n",
      "test 150 times, loss: 203.4231115579605\n",
      "Accuracy: 0.5179166793823242\n",
      "No.5 round trainning\n",
      "trained 7500 times, loss: 1.240898847579956\n",
      "test 50 times, loss: 63.749324560165405\n",
      "Accuracy: 0.5459374785423279\n",
      "test 100 times, loss: 127.05254864692688\n",
      "Accuracy: 0.5496875047683716\n",
      "test 150 times, loss: 191.8318316936493\n",
      "Accuracy: 0.54666668176651\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    print(\"No.{} round trainning\".format(i+1))\n",
    "    \n",
    "    # Train\n",
    "    for data in train_dataloader:\n",
    "        imgs, target = data\n",
    "        if torch.cuda.is_available():                # if GPU is available -> use GPU for trainning\n",
    "            imgs = imgs.cuda()\n",
    "            target = target.cuda()\n",
    "        output = CIFAR10Model(imgs)\n",
    "        loss = loss_fn(output, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_step += 1\n",
    "        \n",
    "        if total_train_step % 500 ==0 :\n",
    "            print(\"trained {} times, loss: {}\".format(total_train_step, loss.item()))\n",
    "            writer.add_scalar(\"train_loss\", loss.item(), total_train_step)\n",
    "            \n",
    "    # Test\n",
    "    with torch.no_grad():     # torch.no_grad() = This process no need to calculate gradient\n",
    "        total_accuracy = 0\n",
    "        total_test_step = 0\n",
    "        total_test_loss = 0\n",
    "        \n",
    "        for data in test_dataloader:\n",
    "            imgs, targets = data\n",
    "            if torch.cuda.is_available():                # if GPU is available -> use GPU for trainning\n",
    "                imgs = imgs.cuda()\n",
    "                targets = targets.cuda()\n",
    "            \n",
    "            outputs = CIFAR10Model(imgs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            total_test_loss += loss.item()\n",
    "            total_test_step += 1\n",
    "            \n",
    "            accuracy = (outputs.argmax(1) == targets).sum()/batchsize   # Calculate the accuracy for a batch i.e. 64 images\n",
    "            total_accuracy += accuracy\n",
    "            \n",
    "            if total_test_step % 50 ==0 :\n",
    "                print(\"test {} times, loss: {}\".format(total_test_step, total_test_loss))\n",
    "                writer.add_scalar(\"test_loss\", total_test_loss, total_test_step)\n",
    "                print(\"Accuracy: {}\".format(total_accuracy/total_test_step))\n",
    "                writer.add_scalar(\"Accuracy\", total_accuracy/total_test_step, total_test_step)\n",
    "\n",
    "writer.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(CIFAR10Model, \"CIFAR10Model_trained.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "tensor([[-0.0790, -1.1381,  0.9373,  0.8728,  0.9242,  0.6612,  0.6203, -0.3582,\n",
      "         -0.6918, -1.3039],\n",
      "        [ 3.2277,  0.5355, -0.2017, -1.2475, -0.5203, -2.6708, -1.4253, -2.9033,\n",
      "          4.2837,  1.1920],\n",
      "        [-0.9420, -3.5167,  0.0543,  1.7124, -0.0166,  2.1941,  0.0633,  2.8640,\n",
      "         -1.7983, -0.5040],\n",
      "        [-0.5109, -2.2419,  1.7499,  1.8696,  0.9789,  1.8819,  1.1167,  0.5312,\n",
      "         -2.7766, -1.7566],\n",
      "        [-0.8496, -3.0060,  0.7435,  1.8666,  1.3408,  1.0453,  1.7055,  0.0287,\n",
      "         -1.0541, -1.6217],\n",
      "        [ 2.0265,  1.4208, -1.7272,  0.4955, -1.1834,  0.4695, -2.3182,  1.7397,\n",
      "         -0.8777,  0.3401],\n",
      "        [-0.0994, -4.2953,  2.7156,  2.6465,  0.9771,  1.9060, -0.2006,  0.6467,\n",
      "         -1.1107, -2.4073],\n",
      "        [ 0.4021, -2.9849,  1.2452,  3.1194, -0.2793,  3.2735, -1.4913,  1.1574,\n",
      "         -1.1062, -2.1964],\n",
      "        [ 0.9762, -0.1523,  0.2748, -0.7890,  0.7620, -1.1369, -0.7303,  0.9316,\n",
      "          0.2289, -0.1232],\n",
      "        [ 0.5630,  3.2162, -0.1521,  0.2640, -1.7204, -0.2509, -2.7581, -1.8987,\n",
      "          2.3600,  0.7584],\n",
      "        [ 0.3366, -1.7120,  2.2350,  1.4870,  0.7917,  0.3237,  0.2779, -3.1435,\n",
      "          1.7088, -1.7555],\n",
      "        [ 4.0337,  1.7962, -0.2842, -1.6186, -1.2007, -3.1318, -3.5270, -4.1570,\n",
      "          6.9045,  1.7616],\n",
      "        [-0.8992, -2.9583,  3.2868,  2.7420,  3.4484,  2.5086,  2.4151,  2.5132,\n",
      "         -8.3016, -3.1096],\n",
      "        [ 0.5705,  2.1651, -1.1730, -0.7170, -2.5663, -1.8671, -3.1621, -0.1342,\n",
      "          1.9429,  3.5317],\n",
      "        [ 2.5773,  1.3592, -0.3235, -1.2232, -0.9626, -1.8716, -2.9770, -2.4848,\n",
      "          4.5793,  1.4891],\n",
      "        [ 1.1162, -0.8411, -0.7561,  0.1476, -0.6145,  0.4357, -0.9951,  1.8103,\n",
      "         -0.6343,  0.7923]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(len(targets))\n",
    "print(len(target))\n",
    "print(len(outputs))\n",
    "print(len(output))\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5bdb8ac3f2d9547dea6fa3bfa2ee8baa04424e0c758556039bba6af79e1d77ff"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
